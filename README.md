# ğŸ“„ Information Retrieval from Multiple PDFs using Google GenAI

A conversational **RAG (Retrieval-Augmented Generation)** system that allows users to upload multiple PDF files and ask questions about their content.  
Built with **LangChain, FAISS, Google GenAI, and Streamlit**.

## ğŸš€ Features

- ğŸ“¤ Upload multiple PDF files  
- ğŸ“š Extract and preprocess text  
- ğŸ” Split text into meaningful chunks for high-quality retrieval  
- ğŸ§  Generate vector embeddings using Google GenAI  
- âš¡ Store and search embeddings using FAISS  
- ğŸ’¬ Conversational multi-turn Q&A with a full RAG pipeline  
- ğŸŒ Clean and interactive Streamlit UI  

## ğŸ› ï¸ Tech Stack

| Component           | Purpose                                  |
|--------------------|-------------------------------------------|
| Python             | Core programming language                 |
| Streamlit          | Web interface for chat + file uploads     |
| LangChain          | Text splitting, embeddings, RAG pipeline  |
| Google GenAI API   | LLM + embeddings generation               |
| FAISS              | Vector database for fast retrieval        |

## ğŸ—ï¸ How to Run the Project

Follow these steps to run the project locally:

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/Information-Retrieval-System

cd Information-Retrieval-System

```

### 2ï¸âƒ£ Create and activate a Conda environment
```bash
conda create -n llmapp python=3.8 -y
conda activate llmapp
```

### 3ï¸âƒ£ Install required dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Add your environment variables

Create a `.env` file in the project root:

```
GOOGLE_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### 5ï¸âƒ£ Run the Streamlit app
```bash
streamlit run app.py
```

### 6ï¸âƒ£ Open in browser

Visit:  
http://localhost:8501

## ğŸ“¦ Project Workflow

1. Upload PDFs  
2. Extract raw text  
3. Split text into smaller chunks  
4. Generate embeddings using Google GenAI  
5. Store vectors in FAISS  
6. Perform semantic search + conversational querying  
7. LLM generates human-like answers with retrieved context  
